{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore') ## seaborn throws warnings that we want to ignore.\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from src.datahandling.loading import load_data  # loading\n",
    "import src.datahandling.visualization as vis  # plotting\n",
    "from src.models.ma import MovingAverage  # model\n",
    "from src.datahandling.preprocessing import (\n",
    "    DataCleaner,\n",
    "    SupervisedTransformer,\n",
    "    train_test_split,\n",
    ")  # preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Moving Average\n",
    "\n",
    "The _Moving Average_ model, the most basic model within this project, operates on the principle that the value of the next time step is computed as the average of the preceding $n$ values. In future we also refer to $n$ as _\"window size\"_. Furthermore, the model is expanded by incorporating a discount factor $\\gamma \\in (0,1]$, which assigns varying weights to these values. Consequently, it closely resembles the concept of discounting employed in determining future rewards within reinforcement learning. The formula for the _Moving Average_ model is expressed as follows:\n",
    "\n",
    "$$\\hat{y}_ {t+1} = \\tfrac{1}{n} \\sum_ {i=0}^{n-1} \\gamma^i \\cdot y_ {t-i}$$\n",
    "\n",
    "Note that by setting $n = 1$ the model reduces to a very naive model that simply predicts the value of the current time step. This can be seen as the most simplistic approach to the problem and will be therefore be referred to as baseline model.\n",
    "Both, $n$ and $\\gamma$ are hyperparamaters that will be tuned for each dataset individually in the following sections.\n",
    "\n",
    "The model is implemented in the file `src/models/ma.py`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the model\n",
    "model = Pipeline(\n",
    "    [\n",
    "        (\"st\", SupervisedTransformer()),\n",
    "        (\"ma\", MovingAverage()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    \"st__horizon\": [\"10min\", \"hourly\", \"daily\"],\n",
    "    \"st__window_size\": [1, 5, 10, 100, 200],\n",
    "    \"ma__discount\": np.round(np.linspace(0.5, 1, 10), 2),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## British Dataset - Kelmarsh\n",
    "\n",
    "For a description and analysis of the dataset, please refer to the notebook `DataInspection.ipynb`. \n",
    "\n",
    "The dataset is loaded and cleaned in the following cell. As the _Moving Average_ model only relies on past values of the target variable and not on any other features, we only need to load the target variable _Power (kWh)_. The SupervisedTransformer is then used to transform the most recent #window size values into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loading data\n",
    "TURBINE_ID = 2\n",
    "data_british = load_data(turbine_id=TURBINE_ID, which_data=\"British\")\n",
    "\n",
    "### cleaning data\n",
    "cleaner = DataCleaner(features=[\"Power (kW)\"], rename_features=[\"power\"])\n",
    "data_british = cleaner.transform(data_british)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyperparameter tuning\n",
    "### WARNING: This takes a long time to run! ~14 min on my machine\n",
    "do_gridsearch = False\n",
    "\n",
    "X = data_british\n",
    "y = data_british[\"power\"]\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=TimeSeriesSplit(n_splits=5),\n",
    "    verbose=1,\n",
    "    refit=False,\n",
    ")\n",
    "\n",
    "if do_gridsearch:\n",
    "    grid_search.fit(X, y)\n",
    "    res_gs_brit = pd.DataFrame(grid_search.cv_results_)\n",
    "    ### store results in csv-file\n",
    "    pd.DataFrame(res_gs_brit).to_csv(\"../results/results_gs_ma_brit.csv\")\n",
    "else:\n",
    "    res_gs_brit = pd.read_csv(\"../results/results_gs_ma_brit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let have a look at the results of the performed gridsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for horizon '10min': {'ma__discount': 0.5, 'st__horizon': '10min', 'st__window_size': 1}\n",
      "Best parameters for horizon 'daily': {'ma__discount': 1.0, 'st__horizon': 'daily', 'st__window_size': 200}\n",
      "Best parameters for horizon 'hourly': {'ma__discount': 0.56, 'st__horizon': 'hourly', 'st__window_size': 100}\n"
     ]
    }
   ],
   "source": [
    "### extract best parameters of each model for each horizon\n",
    "best_params = (\n",
    "    res_gs_brit.set_index([\"params\"])\n",
    "    .groupby(\"param_st__horizon\")\n",
    "    .idxmax()[\"mean_test_score\"]\n",
    ")\n",
    "\n",
    "for horizon in best_params.index:\n",
    "    print(f\"Best parameters for horizon '{horizon}': {best_params[horizon]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-684.2681264197043"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### create models and refit them with best parameters\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brazilian Dataset - Beberine\n",
    "\n",
    "We follow the same procedure as for the British dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loading data\n",
    "TURBINE_ID = 1\n",
    "data_braz = load_data(turbine_id=TURBINE_ID, which_data=\"Brazilian\")\n",
    "\n",
    "### cleaning data\n",
    "cleaner = DataCleaner(features=[\"active_power_total\"], rename_features=[\"power\"])\n",
    "data_braz = cleaner.transform(data_braz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 150 candidates, totalling 750 fits\n"
     ]
    }
   ],
   "source": [
    "### hyperparameter tuning\n",
    "### RUNTIME: ~ 3min on my machine\n",
    "do_gridsearch = False\n",
    "\n",
    "X = data_braz\n",
    "y = data_braz[\"power\"]\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=TimeSeriesSplit(n_splits=5),\n",
    "    verbose=1,\n",
    "    refit=False,\n",
    ")\n",
    "if do_gridsearch:\n",
    "    grid_search.fit(X, y)\n",
    "    res_gs_braz = pd.DataFrame(grid_search.cv_results_)\n",
    "    # store results in csv-file\n",
    "    pd.DataFrame(res_gs_braz).to_csv(\"../results/results_gs_ma_braz.csv\")\n",
    "else:\n",
    "    # load results\n",
    "    res_gs_braz = pd.read_csv(\"../results/results_gs_ma_braz.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Approach\n",
    "\n",
    "Since there is not really the need for fitting the model, i.e. no parameters have to be learned, this model can be seen as already fulfilling the transfer learning challenge because of its simplicity. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "res_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
